{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca5e1df",
   "metadata": {},
   "source": [
    "## Start part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c01e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, TextVectorization, Embedding, Bidirectional\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fca5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "SEQ_LENGTH    = 600\n",
    "MAX_TOKENS    = 20_000\n",
    "\n",
    "PATH_DATASET = r\"D:\\BigDataSets\\RNN\\IMDB_50k.csv\"\n",
    "PATH_GLOVE   = r\"D:\\BigDataSets\\Vectorization\\GloVe\\glove.6B.100d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50d322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n",
      "Loaded texts\n"
     ]
    }
   ],
   "source": [
    "def remove_URL(text):  # to remove URLs\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "def remove_html(text):  # to remove html tags\n",
    "    html = re.compile(r'<.*?>')\n",
    "    return html.sub(r'', text)\n",
    "\n",
    "data_frame = pd.read_csv(PATH_DATASET)\n",
    "data_frame['review'] = data_frame['review'].apply(remove_URL)\n",
    "data_frame['review'] = data_frame['review'].apply(remove_html)\n",
    "\n",
    "data = data_frame.to_numpy()\n",
    "texts, labels = data[:, 0], data[:, 1]\n",
    "labels[labels=='positive'] = \"0\"\n",
    "labels[labels=='negative'] = \"1\"\n",
    "print(data.shape, 'Loaded texts', sep='\\n')\n",
    "# print(labels[:15], texts[:1])\n",
    "\n",
    "# 30_000 - 5_000 - 15_000\n",
    "train_texts, val_texts, test_texts = texts[:30_000], texts[30_000:35_000], texts[35_000:]\n",
    "train_labels, val_labels, test_labels = labels[:30_000], labels[30_000:35_000], labels[35_000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec3fe1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    output_sequence_length=SEQ_LENGTH,\n",
    "    output_mode=\"int\", ngrams=None)\n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_texts)\n",
    "text_vectorization.adapt(text_ds.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31ab374a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "20000\n",
      "tf.Tensor(\n",
      "[ 4474     2  1155     7    28     5   141  2574    95   116     2  1165\n",
      "     7   444   185     2 17920     5   265   240    70   757     1    88\n",
      "     9    13    51  1047     3   176   154    18    14     2    17  7830\n",
      "    11   149   156     2   621    39 17920   154     1     4   468   382\n",
      "    19   174   107     4   425     8   395    48    67    46   176   209\n",
      "    99    18   849    11    40   424     1   808    10    19    56  1260\n",
      "     6     4 10294    35     7  3567     1   137   712    18   123   361\n",
      "   647    36   160  1280     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(type(train_texts))\n",
    "print(text_vectorization.vocabulary_size())\n",
    "print(text_vectorization(train_texts[10]))\n",
    "# print(text_vectorization.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29183b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  28    5    2 ...    0    0    0]\n",
      " [   4  374  112 ...    0    0    0]\n",
      " [  11  191   10 ...    0    0    0]\n",
      " ...\n",
      " [ 198   40  277 ...    0    0    0]\n",
      " [  38  646    1 ...    0    0    0]\n",
      " [1427  105  342 ...    0    0    0]], shape=(64, 600), dtype=int64)\n",
      "tf.Tensor(\n",
      "[0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 0 1 1 1\n",
      " 1 0 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 1 0 1], shape=(64,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "def make_ds(x, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.map(lambda x, y: (text_vectorization(x), tf.strings.to_number(y, out_type=tf.int32)),\n",
    "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE).cache()\n",
    "    return ds\n",
    "\n",
    "train_ds = make_ds(train_texts, train_labels)\n",
    "val_ds = make_ds(val_texts, val_labels)\n",
    "test_ds = make_ds(test_texts, test_labels)\n",
    "\n",
    "for data in train_ds.take(1):\n",
    "    print(*data, sep='\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3da8c",
   "metadata": {},
   "source": [
    "#### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10206922",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(embeddings_index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m word vectors.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 2\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m vocabulary \u001b[38;5;241m=\u001b[39m \u001b[43mtext_vectorization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m word_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(vocabulary, \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(vocabulary))))\n\u001b[0;32m     14\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((MAX_TOKENS, EMBEDDING_DIM))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:487\u001b[0m, in \u001b[0;36mTextVectorization.get_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocabulary\u001b[39m(\u001b[38;5;28mself\u001b[39m, include_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the current vocabulary of the layer.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03m        returned vocabulary will not include any padding or OOV tokens.\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lookup_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\preprocessing\\index_lookup.py:385\u001b[0m, in \u001b[0;36mIndexLookup.get_vocabulary\u001b[1;34m(self, include_special_tokens)\u001b[0m\n\u001b[0;32m    382\u001b[0m     keys, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup_table\u001b[38;5;241m.\u001b[39mexport()\n\u001b[0;32m    383\u001b[0m     vocab, indices \u001b[38;5;241m=\u001b[39m (values, keys) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvert \u001b[38;5;28;01melse\u001b[39;00m (keys, values)\n\u001b[0;32m    384\u001b[0m     vocab, indices \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_vocab_to_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    386\u001b[0m         indices\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[0;32m    387\u001b[0m     )\n\u001b[0;32m    388\u001b[0m lookup \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moov_token, \u001b[38;5;28mzip\u001b[39m(indices, vocab)\n\u001b[0;32m    390\u001b[0m )\n\u001b[0;32m    391\u001b[0m vocab \u001b[38;5;241m=\u001b[39m [lookup[x] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocabulary_size())]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:416\u001b[0m, in \u001b[0;36mStringLookup._tensor_vocab_to_numpy\u001b[1;34m(self, vocabulary)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tensor_vocab_to_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    414\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m--> 416\u001b[0m         [tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_text(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocabulary]\n\u001b[0;32m    417\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\layers\\preprocessing\\string_lookup.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_tensor_vocab_to_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocabulary):\n\u001b[0;32m    414\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m vocabulary\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m--> 416\u001b[0m         [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m vocabulary]\n\u001b[0;32m    417\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\compat.py:110\u001b[0m, in \u001b[0;36mas_text\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m    108\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m bytes_or_text\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bytes_or_text, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m--> 110\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbytes_or_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected binary or unicode string, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m bytes_or_text)\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc3 in position 0: unexpected end of data"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(PATH_GLOVE, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219046cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "embedding_matrix = np.zeros((MAX_TOKENS, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:  # Else zeros\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        print(f\"Cant find {word} in GloVe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671bb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "embedding_layer = Embedding(\n",
    "    MAX_TOKENS, EMBEDDING_DIM,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False, mask_zero=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
